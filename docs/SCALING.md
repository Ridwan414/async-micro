# Kubernetes Autoscaling Guide

This document provides comprehensive guidance for implementing autoscaling in the async-micro application on a single VM Kubernetes cluster.

## Table of Contents

1. [Overview](#overview)
2. [Scaling Fundamentals](#scaling-fundamentals)
   - [What is Scaling?](#what-is-scaling)
   - [Types of Scaling](#types-of-scaling)
   - [Horizontal vs Vertical Scaling](#horizontal-vs-vertical-scaling-comparison)
   - [When to Use Each Scaling Type](#when-to-use-each-scaling-type)
   - [Hybrid Scaling Strategies](#hybrid-scaling-strategies)
3. [Kubernetes Autoscaling Types](#kubernetes-autoscaling-types)
   - [HPA (Horizontal Pod Autoscaler)](#horizontal-pod-autoscaler-hpa)
   - [VPA (Vertical Pod Autoscaler)](#vertical-pod-autoscaler-vpa)
   - [Cluster Autoscaler](#cluster-autoscaler)
   - [Choosing the Right Autoscaler](#choosing-the-right-autoscaler)
4. [Why HPA for This Project?](#why-hpa-for-this-project)
   - [Project Context Analysis](#project-context-analysis)
   - [Decision Matrix](#decision-matrix-why-hpa-wins)
   - [Detailed Reasoning](#detailed-reasoning)
   - [When VPA Would Be Better](#when-vpa-would-be-better)
   - [When Cluster Autoscaler Would Be Better](#when-cluster-autoscaler-would-be-better)
5. [Prerequisites](#prerequisites)
6. [Architecture](#architecture)
7. [Installation](#installation)
8. [Configuration](#configuration)
9. [Scaling Strategies](#scaling-strategies)
10. [Monitoring](#monitoring)
11. [Troubleshooting](#troubleshooting)
12. [Best Practices](#best-practices)
    - [Testing Autoscaling](#5-testing-autoscaling)
    - [Verified Test Results](#verified-test-results)
    - [Understanding Test Results in Detail](#understanding-the-test-results-in-detail)
13. [Quick Reference](#quick-reference)
    - [Kind Cluster Setup](#kind-cluster-setup)
14. [Monitoring with ArgoCD](#monitoring-with-argocd)
    - [Accessing ArgoCD UI](#accessing-argocd-ui)
    - [Viewing HPA in ArgoCD](#viewing-hpa-in-argocd)
    - [ArgoCD CLI Commands](#argocd-cli-commands)

---

## Overview

### What is Autoscaling?

Autoscaling automatically adjusts the number of running pods based on observed metrics like CPU utilization, memory usage, or custom metrics. This ensures your application can handle varying loads while optimizing resource usage.

---

## Scaling Fundamentals

### What is Scaling?

Scaling is the process of adjusting computing resources to meet varying workload demands. It ensures applications maintain performance during traffic spikes while optimizing costs during low-demand periods.

**Key Objectives of Scaling:**
- **Availability**: Ensure the application remains accessible under varying loads
- **Performance**: Maintain acceptable response times and throughput
- **Cost Efficiency**: Use only the resources needed at any given time
- **Fault Tolerance**: Distribute workload to prevent single points of failure

### Types of Scaling

There are two fundamental approaches to scaling: **Horizontal** and **Vertical**.

#### Horizontal Scaling (Scaling Out/In)

Horizontal scaling involves adding or removing instances (pods, containers, VMs) of an application.

```
Before Scaling:          After Scaling Out:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App    â”‚     â†’       â”‚  App    â”‚ â”‚  App    â”‚ â”‚  App    â”‚
â”‚ Instanceâ”‚             â”‚Instance1â”‚ â”‚Instance2â”‚ â”‚Instance3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                        â†“          â†“          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load   â”‚             â”‚         Load Balancer           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Characteristics:**
| Aspect | Description |
|--------|-------------|
| **Mechanism** | Add/remove identical instances |
| **Downtime** | Zero downtime (instances added alongside existing ones) |
| **Fault Tolerance** | High (failure of one instance doesn't affect others) |
| **Complexity** | Requires load balancing and stateless design |
| **Cost Model** | Linear (pay per instance) |
| **Upper Limit** | Theoretically unlimited |

**Best Suited For:**
- Stateless applications (REST APIs, web servers)
- Microservices architectures
- Read-heavy workloads
- Applications with unpredictable traffic patterns

**Challenges:**
- Requires stateless application design or external state management
- Session management complexity (sticky sessions or distributed sessions)
- Data consistency across instances
- Increased operational complexity

#### Vertical Scaling (Scaling Up/Down)

Vertical scaling involves increasing or decreasing the resources (CPU, RAM, storage) of an existing instance.

```
Before Scaling:          After Scaling Up:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    App      â”‚         â”‚         App         â”‚
â”‚  2 CPU      â”‚    â†’    â”‚       8 CPU         â”‚
â”‚  4GB RAM    â”‚         â”‚      32GB RAM       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Characteristics:**
| Aspect | Description |
|--------|-------------|
| **Mechanism** | Increase/decrease resources of single instance |
| **Downtime** | Usually requires restart (except for hot-add in some systems) |
| **Fault Tolerance** | Low (single point of failure) |
| **Complexity** | Simple (no load balancing needed) |
| **Cost Model** | Non-linear (larger instances cost more per unit) |
| **Upper Limit** | Hardware limits of largest available instance |

**Best Suited For:**
- Stateful applications (databases, legacy systems)
- Applications with vertical performance requirements (large in-memory processing)
- Workloads that are difficult to distribute
- Quick fixes for capacity issues

**Challenges:**
- Physical hardware limits
- Usually requires downtime for changes
- Single point of failure
- Diminishing returns at higher resource levels

### Horizontal vs Vertical Scaling: Comparison

| Criteria | Horizontal Scaling | Vertical Scaling |
|----------|-------------------|------------------|
| **Scalability** | Nearly unlimited | Limited by hardware |
| **Downtime** | None | Usually required |
| **Fault Tolerance** | High (redundant instances) | Low (single instance) |
| **Cost at Scale** | More efficient | Expensive (premium for large instances) |
| **Complexity** | Higher (distributed systems) | Lower (single system) |
| **Data Consistency** | Challenging (eventual consistency) | Simple (single source) |
| **State Management** | Requires external state | State can be local |
| **Recovery Time** | Fast (other instances available) | Slow (full restart needed) |

### When to Use Each Scaling Type

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        Scaling Decision Tree        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                                 â–¼
            Is application                    Is application
              stateless?                        stateful?
                    â”‚                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼               â–¼                  â–¼
       Yes                 No            Database          Legacy App
         â”‚                  â”‚               â”‚                  â”‚
         â–¼                  â–¼               â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Can it be       Consider           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Horizontalâ”‚      refactored?     read replicas      â”‚ Vertical â”‚
   â”‚ Scaling  â”‚           â”‚          + write primary    â”‚ Scaling  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼           â–¼         â–¼
                   Yes         No    Horizontal for
                    â”‚           â”‚    reads, Vertical
                    â–¼           â–¼    for writes
              Refactor to  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              stateless    â”‚ Vertical â”‚
                    â”‚      â”‚ Scaling  â”‚
                    â–¼      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Horizontalâ”‚
              â”‚ Scaling  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid Scaling Strategies

In practice, most production systems use a combination of both approaches:

**1. Diagonal Scaling**
Start with vertical scaling for quick wins, then switch to horizontal scaling for long-term growth.

```
Phase 1 (Quick Fix):     Phase 2 (Scale Out):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Larger      â”‚   â†’    â”‚ Medium  â”‚ â”‚ Medium  â”‚ â”‚ Medium  â”‚
â”‚   Instance    â”‚        â”‚Instance1â”‚ â”‚Instance2â”‚ â”‚Instance3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Tiered Scaling**
Different scaling strategies for different components:

| Component | Scaling Strategy | Rationale |
|-----------|-----------------|-----------|
| Web Servers | Horizontal | Stateless, easy to replicate |
| Application Servers | Horizontal | Stateless with external session store |
| Cache (Redis) | Horizontal (Cluster) | Read distribution |
| Database Primary | Vertical | Write consistency |
| Database Replicas | Horizontal | Read distribution |

**3. Auto-Scaling with Limits**
Combine automated horizontal scaling with vertical resource optimization:

```yaml
# Example: HPA with VPA recommendations
HPA: Scale pods 2-10 based on CPU
VPA: Recommend optimal CPU/memory per pod
Result: Right-sized pods that scale horizontally
```

---

## Kubernetes Autoscaling Types

Kubernetes provides three main autoscaling mechanisms:

### Types of Autoscaling

| Type | Description | Use Case |
|------|-------------|----------|
| **HPA** (Horizontal Pod Autoscaler) | Scales the number of pod replicas | Stateless workloads |
| **VPA** (Vertical Pod Autoscaler) | Adjusts CPU/memory requests | Right-sizing containers |
| **Cluster Autoscaler** | Adds/removes nodes | Multi-node clusters |

### Horizontal Pod Autoscaler (HPA)

HPA automatically scales the number of pod replicas based on observed metrics.

**How HPA Works:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Metrics      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Metrics    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚     HPA      â”‚
â”‚   Server     â”‚                  â”‚  Controller  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                                 â”‚
       â”‚ Collect                         â”‚ Scale
       â”‚ metrics                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Pods      â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Deployment  â”‚
â”‚  (replicas)  â”‚    Adjust count  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scaling Algorithm:**
```
desiredReplicas = ceil[currentReplicas Ã— (currentMetricValue / desiredMetricValue)]
```

Example: If current CPU = 80%, target = 50%, and replicas = 2:
```
desiredReplicas = ceil[2 Ã— (80/50)] = ceil[3.2] = 4
```

**Pros:**
- Zero downtime scaling
- Works with any stateless workload
- Supports multiple metrics (CPU, memory, custom)
- Native Kubernetes resource

**Cons:**
- Requires stateless applications
- Cold start latency for new pods
- Reactive (scales after load increases)

### Vertical Pod Autoscaler (VPA)

VPA automatically adjusts CPU and memory requests/limits for containers.

**How VPA Works:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Resource      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Metrics    â”‚    Utilization    â”‚     VPA      â”‚
â”‚   History    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  Recommender â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â”‚ Recommendations
                                          â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   Updater    â”‚
                                   â”‚  (optional)  â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚ Evict & recreate
                                          â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚    Pods      â”‚
                                   â”‚ (new limits) â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VPA Modes:**
| Mode | Behavior |
|------|----------|
| **Off** | Only provides recommendations, no action |
| **Initial** | Applies recommendations only at pod creation |
| **Auto** | Updates running pods (may cause restarts) |

**Pros:**
- Optimal resource utilization
- Reduces over-provisioning waste
- Helps with initial resource estimation

**Cons:**
- Pod restarts when updating resources
- Cannot be used with HPA on same metrics
- Slower to react than HPA

### Cluster Autoscaler

Cluster Autoscaler adjusts the number of nodes in a cluster.

**How Cluster Autoscaler Works:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Node1  â”‚ â”‚  Node2  â”‚ â”‚  Node3  â”‚ ... â”‚ NodeN   â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚     â”‚ (new)   â”‚   â”‚
â”‚  â”‚ (full)  â”‚ â”‚ (full)  â”‚ â”‚ (full)  â”‚     â”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â–²                                        â–²        â”‚
â”‚       â”‚         Cluster Autoscaler             â”‚        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                        â”‚                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚ Pending Pods?     â”‚                      â”‚
â”‚              â”‚ Underutilized?    â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Cloud Provider    â”‚
              â”‚   (Add/Remove VMs)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scale Up Trigger:** Pods are pending due to insufficient resources
**Scale Down Trigger:** Node utilization below threshold for extended period

**Pros:**
- Handles cluster-level resource constraints
- Works with cloud provider APIs
- Complements HPA and VPA

**Cons:**
- Only for multi-node cloud clusters
- Slower than pod scaling (minutes)
- Not applicable to single VM setups

### Choosing the Right Autoscaler

| Scenario | Recommended Autoscaler |
|----------|----------------------|
| Stateless web application with variable traffic | HPA |
| Long-running batch jobs with unknown resource needs | VPA |
| Multi-node cloud cluster with HPA workloads | HPA + Cluster Autoscaler |
| Single VM/node deployment | HPA only |
| Mixed workloads (stateless + stateful) | HPA for stateless, VPA for stateful |
| Initial deployment (unknown resource requirements) | VPA in "Off" mode for recommendations |

### HPA vs VPA vs Cluster Autoscaler

| Feature | HPA | VPA | Cluster Autoscaler |
|---------|-----|-----|-------------------|
| **Scales** | Pod count | Pod resources | Node count |
| **Direction** | Horizontal | Vertical | Horizontal (nodes) |
| **Latency** | Seconds | Minutes (restart) | Minutes |
| **Downtime** | None | Pod restart | None |
| **Metrics** | CPU, memory, custom | CPU, memory | Pending pods |
| **Best For** | Stateless apps | Right-sizing | Cloud clusters |
| **Single VM** | Yes | Yes | No |

This guide focuses on **HPA** as it's the most suitable for single VM deployments.

---

## Why HPA for This Project?

This section explains in detail why **Horizontal Pod Autoscaler (HPA)** was chosen over VPA and Cluster Autoscaler for the async-micro application.

### Project Context Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ASYNC-MICRO PROJECT CONTEXT                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Deployment Environment: Single VM (Minikube/K3s/MicroK8s)              â”‚
â”‚  Application Type: Microservices (Gateway, API, Worker, Product)        â”‚
â”‚  State Management: Stateless services + External state (RabbitMQ)       â”‚
â”‚  Traffic Pattern: Variable, unpredictable user requests                 â”‚
â”‚  Priority: Zero downtime, fast response to load changes                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Best Fit: HPA (Horizontal    â”‚
                    â”‚       Pod Autoscaler)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Matrix: Why HPA Wins

| Evaluation Criteria | HPA | VPA | Cluster Autoscaler | Winner |
|---------------------|-----|-----|-------------------|--------|
| **Single VM Compatible** | Yes | Yes | No (requires cloud) | HPA/VPA |
| **Zero Downtime Scaling** | Yes | No (pod restart) | Yes | HPA |
| **Scaling Speed** | Seconds | Minutes | Minutes | HPA |
| **Stateless Workloads** | Excellent | Good | N/A | HPA |
| **Native K8s Support** | Built-in | Requires install | Requires cloud | HPA |
| **Operational Complexity** | Low | Medium | High | HPA |
| **Resource Efficiency** | Good | Excellent | Good | VPA |

**Final Score: HPA wins 5/7 categories**

### Detailed Reasoning

#### 1. Single VM Deployment Constraint

The async-micro project runs on a **single VM** using lightweight Kubernetes distributions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SINGLE VM                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Kubernetes (K3s/Minikube)               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Gateway â”‚ â”‚   API   â”‚ â”‚ Worker  â”‚ â”‚ Product â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  Pods   â”‚ â”‚  Pods   â”‚ â”‚  Pods   â”‚ â”‚  Pods   â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  âŒ Cluster Autoscaler: CANNOT add more VMs                  â”‚
â”‚  âœ… HPA: CAN add more pods within VM resources               â”‚
â”‚  âœ… VPA: CAN adjust pod resources (but requires restarts)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Cluster Autoscaler is eliminated** because:
- Requires cloud provider API (AWS, GCP, Azure) to provision new nodes
- Cannot create additional VMs in a single-VM setup
- Designed for multi-node production clusters

#### 2. Stateless Application Architecture

All services in async-micro are designed to be **stateless**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION STATE ANALYSIS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  STATELESS SERVICES (Perfect for HPA):                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Gateway   â”‚ â”‚     API     â”‚ â”‚   Worker    â”‚ â”‚   Product   â”‚    â”‚
â”‚  â”‚             â”‚ â”‚             â”‚ â”‚             â”‚ â”‚             â”‚    â”‚
â”‚  â”‚ â€¢ No local  â”‚ â”‚ â€¢ No local  â”‚ â”‚ â€¢ No local  â”‚ â”‚ â€¢ No local  â”‚    â”‚
â”‚  â”‚   state     â”‚ â”‚   state     â”‚ â”‚   state     â”‚ â”‚   state     â”‚    â”‚
â”‚  â”‚ â€¢ Any pod   â”‚ â”‚ â€¢ Any pod   â”‚ â”‚ â€¢ Any pod   â”‚ â”‚ â€¢ Any pod   â”‚    â”‚
â”‚  â”‚   can serve â”‚ â”‚   can serve â”‚ â”‚   can serve â”‚ â”‚   can serve â”‚    â”‚
â”‚  â”‚   request   â”‚ â”‚   request   â”‚ â”‚   any job   â”‚ â”‚   request   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        âœ…              âœ…              âœ…              âœ…             â”‚
â”‚       HPA             HPA             HPA             HPA            â”‚
â”‚                                                                       â”‚
â”‚  STATEFUL SERVICE (NOT suitable for HPA):                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚  â”‚  RabbitMQ   â”‚  â€¢ Persistent message queue                         â”‚
â”‚  â”‚             â”‚  â€¢ Requires clustering for HA                       â”‚
â”‚  â”‚ StatefulSet â”‚  â€¢ Uses PersistentVolumeClaim                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚        âŒ                                                             â”‚
â”‚   No autoscaling (kept at 1 replica)                                 â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why stateless = HPA:**
- Pods are interchangeable - any pod can handle any request
- No session affinity required
- Load balancer can distribute evenly
- New pods are immediately useful after startup

**Why RabbitMQ is excluded:**
- Message broker requires persistent storage
- Scaling requires proper clustering configuration
- Data consistency must be maintained

#### 3. Zero Downtime Requirement

Production applications cannot afford downtime during scaling operations:

```
                        HPA Scaling (Zero Downtime)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Time: T0 (2 pods)         Time: T1 (scaling)       Time: T2 (4 pods) â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Pod 1â”‚ â”‚Pod 2â”‚    â†’     â”‚Pod 1â”‚ â”‚Pod 2â”‚    â†’     â”‚Pod 1â”‚ â”‚Pod 2â”‚  â”‚
â”‚  â”‚ âœ…  â”‚ â”‚ âœ…  â”‚          â”‚ âœ…  â”‚ â”‚ âœ…  â”‚          â”‚ âœ…  â”‚ â”‚ âœ…  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚
â”‚                           â”‚Pod 3â”‚ â”‚Pod 4â”‚          â”‚Pod 3â”‚ â”‚Pod 4â”‚  â”‚
â”‚                           â”‚ ðŸ”„  â”‚ â”‚ ðŸ”„  â”‚          â”‚ âœ…  â”‚ â”‚ âœ…  â”‚  â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â”‚  Existing pods continue serving traffic while new pods start          â”‚
â”‚  Result: âœ… ZERO DOWNTIME                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        VPA Scaling (Causes Downtime)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Time: T0 (100m CPU)       Time: T1 (eviction)      Time: T2 (200m CPU) â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Pod 1     â”‚    â†’     â”‚   Pod 1     â”‚    â†’     â”‚   Pod 1     â”‚   â”‚
â”‚  â”‚  100m CPU   â”‚          â”‚ âŒ EVICTED  â”‚          â”‚  200m CPU   â”‚   â”‚
â”‚  â”‚     âœ…      â”‚          â”‚  RESTARTING â”‚          â”‚     âœ…      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                       â”‚
â”‚  Pod must be terminated and recreated with new resource limits        â”‚
â”‚  Result: âŒ DOWNTIME during restart                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VPA causes downtime because:**
- Cannot change resource limits of a running container
- Must evict (terminate) the pod
- Creates new pod with updated limits
- Service disruption during pod restart (30s - 2min typically)

#### 4. Response Time to Load Changes

When traffic spikes, fast scaling is critical:

```
                    Scaling Response Time Comparison

Traffic Spike â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
     â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   â”‚                                                         â”‚
     â–¼   â”‚  HPA Response                                          â”‚
         â”‚  â”œâ”€â”€ Metrics detected: 0s                               â”‚
    â”Œâ”€â”€â”€â”€â”‚  â”œâ”€â”€ Decision made: ~15s                               â”‚
    â”‚    â”‚  â”œâ”€â”€ New pods scheduled: ~5s                           â”‚
    â”‚    â”‚  â”œâ”€â”€ Containers started: ~10-30s                       â”‚
    â”‚    â”‚  â””â”€â”€ Total: 30-60 seconds                              â”‚
    â”‚    â”‚      âœ… Fast enough for most traffic spikes            â”‚
    â”‚    â”‚                                                         â”‚
    â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    â”‚                                                         â”‚
    â”‚    â”‚  VPA Response                                          â”‚
    â”‚    â”‚  â”œâ”€â”€ Metrics analyzed: ~minutes (historical data)      â”‚
    â”‚    â”‚  â”œâ”€â”€ Recommendation generated: ~1-5 min                â”‚
    â”‚    â”‚  â”œâ”€â”€ Pod eviction: ~30s                                â”‚
    â”‚    â”‚  â”œâ”€â”€ New pod startup: ~30s-1min                        â”‚
    â”‚    â”‚  â””â”€â”€ Total: 2-10 minutes                               â”‚
    â”‚    â”‚      âš ï¸ Too slow for real-time traffic spikes          â”‚
    â”‚    â”‚                                                         â”‚
    â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    â”‚                                                         â”‚
    â”‚    â”‚  Cluster Autoscaler Response                           â”‚
    â”‚    â”‚  â”œâ”€â”€ Pending pods detected: ~30s                       â”‚
    â”‚    â”‚  â”œâ”€â”€ Node provisioning: 2-5 min (cloud API)            â”‚
    â”‚    â”‚  â”œâ”€â”€ Node registration: ~1 min                         â”‚
    â”‚    â”‚  â”œâ”€â”€ Pod scheduling: ~30s                              â”‚
    â”‚    â”‚  â””â”€â”€ Total: 4-10 minutes                               â”‚
    â”‚    â”‚      âš ï¸ Designed for capacity, not real-time           â”‚
    â”‚    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5. Operational Simplicity

| Aspect | HPA | VPA | Cluster Autoscaler |
|--------|-----|-----|-------------------|
| **Installation** | Built into K8s | Requires separate install | Cloud-specific setup |
| **Configuration** | Simple YAML | Complex tuning | Cloud IAM + config |
| **Dependencies** | Metrics Server only | Metrics + Admission Controller | Cloud provider API |
| **Monitoring** | `kubectl get hpa` | Custom dashboards | Cloud console + kubectl |
| **Debugging** | `kubectl describe hpa` | Complex (multiple components) | Cloud logs + K8s events |

```bash
# HPA: Simple to verify
kubectl get hpa
kubectl describe hpa api-hpa

# VPA: Multiple components to check
kubectl get vpa
kubectl get pods -n kube-system | grep vpa
kubectl logs -n kube-system vpa-recommender-xxx
kubectl logs -n kube-system vpa-updater-xxx

# Cluster Autoscaler: Cloud + K8s debugging
kubectl logs -n kube-system cluster-autoscaler-xxx
aws autoscaling describe-auto-scaling-groups  # or equivalent
```

### When VPA Would Be Better

VPA is still useful in specific scenarios (not applicable to this project):

| Scenario | Why VPA | Example |
|----------|---------|---------|
| **Unknown resource needs** | Learns optimal sizing | New application deployment |
| **Batch jobs** | Adjusts for each run | ML training jobs |
| **Vertical performance** | Single-threaded apps | Legacy monoliths |
| **Cost optimization** | Right-sizes over-provisioned pods | Reducing resource waste |

**Recommended VPA Usage for async-micro:**
```yaml
# Use VPA in "Off" mode just for recommendations
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  updatePolicy:
    updateMode: "Off"  # Only recommend, don't apply
```

Then check recommendations:
```bash
kubectl describe vpa api-vpa
# Use recommendations to tune HPA resource requests
```

### When Cluster Autoscaler Would Be Better

Cluster Autoscaler becomes relevant when:

| Scenario | Why Cluster Autoscaler |
|----------|----------------------|
| **Multi-node cloud cluster** | Can provision new VMs |
| **HPA hits node limits** | Pods pending due to no node capacity |
| **Cost optimization at scale** | Remove underutilized nodes |
| **Production high availability** | Multi-AZ node distribution |

```
Future Growth Path:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                   â”‚
â”‚   Phase 1 (Current)          Phase 2 (Future)                    â”‚
â”‚   Single VM                   Multi-Node Cloud                    â”‚
â”‚                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚   Single    â”‚     â†’      â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚   â”‚     VM      â”‚            â”‚  â”‚Node1â”‚ â”‚Node2â”‚ â”‚Node3â”‚       â”‚ â”‚
â”‚   â”‚             â”‚            â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚   â”‚  HPA only   â”‚            â”‚                                 â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  HPA + Cluster Autoscaler       â”‚ â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Summary: Why HPA for async-micro

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINAL RECOMMENDATION: HPA                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  âœ… MATCHES PROJECT REQUIREMENTS:                                    â”‚
â”‚     â€¢ Single VM deployment (K3s/Minikube)                           â”‚
â”‚     â€¢ Stateless microservices architecture                          â”‚
â”‚     â€¢ Zero downtime scaling requirement                             â”‚
â”‚     â€¢ Fast response to traffic changes                              â”‚
â”‚     â€¢ Simple operations and debugging                               â”‚
â”‚                                                                      â”‚
â”‚  âŒ VPA NOT CHOSEN BECAUSE:                                         â”‚
â”‚     â€¢ Causes pod restarts (downtime)                                â”‚
â”‚     â€¢ Slower response time                                          â”‚
â”‚     â€¢ Cannot be used with HPA on same metrics                       â”‚
â”‚     â€¢ More complex operational overhead                             â”‚
â”‚                                                                      â”‚
â”‚  âŒ CLUSTER AUTOSCALER NOT CHOSEN BECAUSE:                          â”‚
â”‚     â€¢ Requires multi-node cloud cluster                             â”‚
â”‚     â€¢ Not applicable to single VM setups                            â”‚
â”‚     â€¢ Needs cloud provider integration                              â”‚
â”‚                                                                      â”‚
â”‚  ðŸ“‹ SERVICES USING HPA:                                             â”‚
â”‚     â€¢ Gateway (2-5 replicas)                                        â”‚
â”‚     â€¢ API (2-10 replicas)                                           â”‚
â”‚     â€¢ Worker (2-15 replicas)                                        â”‚
â”‚     â€¢ Product (1-5 replicas)                                        â”‚
â”‚                                                                      â”‚
â”‚  ðŸ“‹ SERVICES NOT USING HPA:                                         â”‚
â”‚     â€¢ RabbitMQ (StatefulSet, 1 replica - requires clustering)       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Prerequisites

### Single VM Kubernetes Options

Choose one of these lightweight Kubernetes distributions:

#### Option 1: K3s (Recommended for Production)

```bash
# Install K3s
curl -sfL https://get.k3s.io | sh -

# Verify installation
sudo k3s kubectl get nodes

# Set kubeconfig
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
```

#### Option 2: Minikube (Development)

```bash
# Install Minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# Start with Docker driver
minikube start --driver=docker --memory=4096 --cpus=2

# Enable metrics-server addon
minikube addons enable metrics-server
```

#### Option 3: MicroK8s (Ubuntu)

```bash
# Install MicroK8s
sudo snap install microk8s --classic

# Enable required addons
microk8s enable dns storage metrics-server

# Alias kubectl
alias kubectl='microk8s kubectl'
```

### VM Resource Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| CPU | 2 cores | 4+ cores |
| RAM | 4 GB | 8+ GB |
| Disk | 20 GB | 50+ GB |

---

## Architecture

### Application Services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KUBERNETES CLUSTER                       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Gateway    â”‚    â”‚     API      â”‚    â”‚   Product    â”‚       â”‚
â”‚  â”‚   (2-5)      â”‚â”€â”€â”€â–¶â”‚   (2-10)     â”‚    â”‚   (1-5)      â”‚       â”‚
â”‚  â”‚   HPA        â”‚    â”‚   HPA        â”‚    â”‚   HPA        â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                             â”‚                                    â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                      â”‚   RabbitMQ   â”‚                           â”‚
â”‚                      â”‚   (1 replica)â”‚                           â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                             â”‚                                    â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                      â”‚    Worker    â”‚                           â”‚
â”‚                      â”‚   (2-15)     â”‚                           â”‚
â”‚                      â”‚   HPA        â”‚                           â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Metrics Server                         â”‚   â”‚
â”‚  â”‚              (Collects CPU/Memory metrics)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scaling Configuration Summary

| Service | Min Replicas | Max Replicas | CPU Target | Memory Target |
|---------|--------------|--------------|------------|---------------|
| Gateway | 2 | 5 | 50% | 70% |
| API | 2 | 10 | 50% | 70% |
| Worker | 2 | 15 | 60% | 70% |
| Product | 1 | 5 | 50% | 70% |

---

## Installation

### Step 1: Install Metrics Server

The Metrics Server is required for HPA to function. It collects resource metrics from kubelets.

```bash
# Apply the metrics-server manifest (includes --kubelet-insecure-tls for single VM)
kubectl apply -f manifests/hpa/metrics-server.yaml

# Wait for metrics-server to be ready
kubectl wait --for=condition=available --timeout=300s deployment/metrics-server -n kube-system

# Verify metrics are available (may take 1-2 minutes)
kubectl top nodes
kubectl top pods
```

**Expected output:**
```
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
minikube   250m         12%    1024Mi          25%
```

### Step 2: Deploy Application with Resource Limits

Resource requests and limits are **required** for HPA to calculate utilization percentages.

```bash
# Apply all deployments (already configured with resources)
kubectl apply -f manifests/api-deployment.yaml
kubectl apply -f manifests/gateway-deployment.yaml
kubectl apply -f manifests/worker-deployment.yaml
kubectl apply -f manifests/product-deployment.yaml

# Verify deployments
kubectl get deployments
```

### Step 3: Apply HPA Configurations

```bash
# Apply all HPA configurations
kubectl apply -f manifests/hpa/

# Verify HPAs are created
kubectl get hpa
```

**Expected output:**
```
NAME          REFERENCE            TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
api-hpa       Deployment/api       10%/50%         2         10        2          1m
gateway-hpa   Deployment/gateway   8%/50%          2         5         2          1m
worker-hpa    Deployment/worker    5%/60%          2         15        3          1m
product-hpa   Deployment/product   12%/50%         1         5         2          1m
```

---

## Configuration

### HPA Manifest Structure

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api                    # Target deployment
  minReplicas: 2                 # Minimum pods (high availability)
  maxReplicas: 10                # Maximum pods (resource limit)
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50   # Scale when CPU > 50%
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70   # Scale when Memory > 70%
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scale down
    scaleUp:
      stabilizationWindowSeconds: 0    # Scale up immediately
```

### Resource Configuration

Each deployment must have resource requests defined:

```yaml
resources:
  requests:
    cpu: "100m"      # 0.1 CPU core (used for HPA calculations)
    memory: "128Mi"  # 128 MB RAM
  limits:
    cpu: "500m"      # 0.5 CPU core maximum
    memory: "256Mi"  # 256 MB RAM maximum
```

### Scaling Behavior Configuration

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300  # Prevents flapping
    policies:
    - type: Percent
      value: 50                      # Scale down max 50% at a time
      periodSeconds: 60
  scaleUp:
    stabilizationWindowSeconds: 0    # React immediately to load
    policies:
    - type: Percent
      value: 100                     # Can double pods
      periodSeconds: 15
    - type: Pods
      value: 4                       # Or add 4 pods
      periodSeconds: 15
    selectPolicy: Max                # Use whichever adds more pods
```

---

## Scaling Strategies

### Strategy 1: CPU-Based Scaling (Default)

Best for: API services, Gateway, compute-intensive workloads

```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 50
```

### Strategy 2: Memory-Based Scaling

Best for: Services with memory-intensive operations

```yaml
metrics:
- type: Resource
  resource:
    name: memory
    target:
      type: Utilization
      averageUtilization: 70
```

### Strategy 3: Combined CPU + Memory (Recommended)

Best for: Production workloads with varying resource patterns

```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 50
- type: Resource
  resource:
    name: memory
    target:
      type: Utilization
      averageUtilization: 70
```

### Strategy 4: Custom Metrics (Advanced)

For queue-based scaling (requires Prometheus Adapter):

```yaml
metrics:
- type: External
  external:
    metric:
      name: rabbitmq_queue_messages
      selector:
        matchLabels:
          queue: task_queue
    target:
      type: AverageValue
      averageValue: 10
```

---

## Monitoring

### Real-Time HPA Monitoring

```bash
# Watch HPA status continuously
kubectl get hpa -w

# Detailed HPA information
kubectl describe hpa api-hpa

# View scaling events
kubectl get events --sort-by='.lastTimestamp' | grep -i scale
```

### Resource Monitoring

```bash
# Current resource usage
kubectl top pods

# Node resource usage
kubectl top nodes

# Detailed pod metrics
kubectl top pods --containers
```

### Grafana Dashboard Queries

If using the included Grafana setup, add these Prometheus queries:

```promql
# Current replicas vs desired
kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler="api-hpa"}
kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler="api-hpa"}

# CPU utilization percentage
sum(rate(container_cpu_usage_seconds_total{pod=~"api-.*"}[5m])) /
sum(kube_pod_container_resource_requests{pod=~"api-.*", resource="cpu"}) * 100

# Memory utilization percentage
sum(container_memory_usage_bytes{pod=~"api-.*"}) /
sum(kube_pod_container_resource_requests{pod=~"api-.*", resource="memory"}) * 100
```

---

## Troubleshooting

### Common Issues

#### 1. HPA Shows `<unknown>` for Targets

**Cause:** Metrics server not running or no resource requests defined.

```bash
# Check metrics server
kubectl get pods -n kube-system | grep metrics-server

# Verify metrics are available
kubectl top pods

# Check deployment has resources defined
kubectl get deployment api -o yaml | grep -A 10 resources
```

**Solution:**
```bash
# Restart metrics server
kubectl rollout restart deployment/metrics-server -n kube-system

# Ensure deployments have resource requests
kubectl apply -f manifests/api-deployment.yaml
```

#### 2. HPA Not Scaling Up

**Cause:** Target utilization not reached or stabilization window.

```bash
# Check current utilization
kubectl describe hpa api-hpa

# Look for scaling events
kubectl get events --field-selector reason=SuccessfulRescale
```

#### 3. HPA Scaling Too Aggressively

**Cause:** Stabilization window too short.

**Solution:** Increase `stabilizationWindowSeconds`:
```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 60  # Wait 1 minute
```

#### 4. Pods Pending After Scale Up

**Cause:** Insufficient node resources.

```bash
# Check node capacity
kubectl describe node | grep -A 10 "Allocated resources"

# Check pending pods
kubectl get pods --field-selector=status.phase=Pending
```

**Solution:** Reduce `maxReplicas` or increase VM resources.

### Diagnostic Commands

```bash
# Complete HPA status
kubectl get hpa -o yaml

# HPA events
kubectl describe hpa api-hpa | grep -A 20 Events

# Pod resource usage history
kubectl top pods --containers

# Check if metrics API is working
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods
```

---

## Best Practices

### 1. Resource Configuration

- **Always set resource requests** - HPA requires them
- **Set limits 2-5x requests** - Allows burst capacity
- **Monitor actual usage** - Adjust based on real metrics

```yaml
resources:
  requests:
    cpu: "100m"      # Start conservative
    memory: "128Mi"
  limits:
    cpu: "500m"      # 5x headroom for bursts
    memory: "512Mi"  # 4x headroom
```

### 2. Replica Limits for Single VM

| VM RAM | Recommended Max Total Pods |
|--------|---------------------------|
| 4 GB | 15-20 pods |
| 8 GB | 30-40 pods |
| 16 GB | 60-80 pods |

**Our Configuration:**
- Gateway: max 5
- API: max 10
- Worker: max 15
- Product: max 5
- **Total max: 35 pods** (suitable for 8GB+ VM)

### 3. Scaling Behavior

```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 0    # React fast to load
    policies:
    - type: Pods
      value: 4
      periodSeconds: 15
  scaleDown:
    stabilizationWindowSeconds: 300  # Prevent flapping (5 min)
    policies:
    - type: Percent
      value: 50
      periodSeconds: 60
```

### 4. Service-Specific Recommendations

| Service | Min | Max | CPU Target | Notes |
|---------|-----|-----|------------|-------|
| Gateway | 2 | 5 | 50% | Entry point, keep stable |
| API | 2 | 10 | 50% | Scales with request load |
| Worker | 2 | 15 | 60% | Scales with queue depth |
| Product | 1 | 5 | 50% | Lower traffic expected |
| RabbitMQ | 1 | 1 | N/A | StatefulSet, don't scale |

### 5. Testing Autoscaling

#### Prerequisites for Testing

Before testing, ensure all components are properly configured:

```bash
# 1. Verify metrics server is running
kubectl get pods -n kube-system | grep metrics-server
# Expected: metrics-server-xxx   1/1   Running

# 2. Verify metrics are being collected
kubectl top nodes
kubectl top pods
# Should show CPU and memory values, not <unknown>

# 3. Verify HPAs are configured and receiving metrics
kubectl get hpa
# Expected: TARGETS should show actual percentages, not <unknown>
```

#### Test Method 1: CPU Stress Test (Recommended)

Stress a pod's CPU directly to trigger scaling:

```bash
# 1. Get the name of an API pod
kubectl get pods -l app=api

# 2. Stress the pod's CPU (runs multiple CPU-intensive processes)
kubectl exec -it <api-pod-name> -- sh -c "for i in 1 2 3 4 5; do yes > /dev/null & done"

# 3. In another terminal, watch the HPA react
kubectl get hpa -w

# 4. Watch pods scale up
kubectl get pods -l app=api -w

# 5. Clean up - delete the stressed pod (deployment will recreate it)
kubectl delete pod <api-pod-name> --grace-period=0 --force
```

#### Test Method 2: HTTP Load Generator

Generate HTTP traffic to simulate real load:

```bash
# Create a load generator pod
kubectl run load-generator --image=busybox:1.36 --restart=Never -- \
  /bin/sh -c "while true; do wget -q -O- http://gateway-service:3000/health; done"

# Watch scaling in action
kubectl get hpa -w

# In another terminal, watch pods
kubectl get pods -w

# Clean up
kubectl delete pod load-generator
```

#### Test Method 3: Heavy Load with Multiple Generators

For more aggressive testing:

```bash
# Create multiple load generators
for i in 1 2 3 4 5; do
  kubectl run load-gen-$i --image=busybox:1.36 --restart=Never -- \
    /bin/sh -c "while true; do wget -q -O- http://api-service:8000/ 2>/dev/null; done"
done

# Watch HPA
kubectl get hpa -w

# Clean up all generators
kubectl delete pod -l run=load-gen-1
kubectl delete pod -l run=load-gen-2
# ... or
kubectl delete pods load-gen-1 load-gen-2 load-gen-3 load-gen-4 load-gen-5
```

#### Verified Test Results

The following comprehensive test was performed on the `microservices-prod` kind cluster on **2025-12-10**.

**Test Environment:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TEST ENVIRONMENT                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Cluster Type:     kind (Kubernetes in Docker)                       â”‚
â”‚  Cluster Name:     microservices-prod                                â”‚
â”‚  Node:             microservices-prod-control-plane                  â”‚
â”‚  Kubernetes:       v1.34.0                                           â”‚
â”‚  Container Runtime: containerd://2.1.3                               â”‚
â”‚  OS:               Debian GNU/Linux 12 (bookworm)                    â”‚
â”‚  Metrics Server:   v0.7.0                                            â”‚
â”‚  Platform:         WSL2 on Windows                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Node Resources:**
```
NAME                               CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
microservices-prod-control-plane   583m         7%       2055Mi          26%
```

---

##### Test 1: API Service Scale-Up and Scale-Down

**Baseline State (08:37:59):**
```
NAME          REFERENCE            TARGETS                        MINPODS   MAXPODS   REPLICAS
api-hpa       Deployment/api       cpu: 1%/50%, memory: 16%/70%   2         10        2
gateway-hpa   Deployment/gateway   cpu: 0%/50%, memory: 26%/70%   2         5         2
product-hpa   Deployment/product   cpu: 1%/50%, memory: 17%/70%   1         5         1
worker-hpa    Deployment/worker    cpu: 1%/60%, memory: 8%/70%    2         15        2

Pod Counts: API=2, Gateway=2, Worker=2, Product=1
```

**Stress Test Started (08:38:12):**
```bash
# Command executed:
kubectl exec api-66bd89b9bf-m24cz -- sh -c "for i in 1 2 3 4 5 6 7 8; do yes > /dev/null & done"
```

**Scale-Up Detected (08:40:37):**
```
NAME      REFERENCE        TARGETS                         MINPODS   MAXPODS   REPLICAS
api-hpa   Deployment/api   cpu: 50%/50%, memory: 16%/70%   2         10        10

Pod Resource Usage:
NAME                   CPU(cores)   MEMORY(bytes)
api-66bd89b9bf-m24cz   500m         22Mi            (stressed pod - hitting limit)
api-66bd89b9bf-nwnr9   1m           21Mi
api-66bd89b9bf-258w5   1m           21Mi            (new)
api-66bd89b9bf-75qsx   1m           21Mi            (new)
... (8 new pods created)
```

**API HPA Scaling Events:**
```
TIMESTAMP   EVENT                 REPLICAS   REASON
08:38:36    SuccessfulRescale    2 â†’ 6      cpu resource utilization above target
08:38:51    SuccessfulRescale    6 â†’ 10     cpu resource utilization above target
```

**Scale-Up Analysis:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API SCALE-UP ANALYSIS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Initial State:                                                      â”‚
â”‚    - Replicas: 2                                                     â”‚
â”‚    - CPU per pod: 1m (1% of 100m request)                           â”‚
â”‚                                                                      â”‚
â”‚  During Stress:                                                      â”‚
â”‚    - Stressed pod CPU: 500m (500% of 100m request, capped at limit) â”‚
â”‚    - Average CPU: (500 + 1) / 2 = 250.5m                            â”‚
â”‚    - Utilization: 250.5 / 100 = 250%                                â”‚
â”‚                                                                      â”‚
â”‚  HPA Calculation:                                                    â”‚
â”‚    desiredReplicas = ceil[2 Ã— (250 / 50)] = ceil[10] = 10           â”‚
â”‚                                                                      â”‚
â”‚  Scaling Behavior:                                                   â”‚
â”‚    - scaleUp.stabilizationWindowSeconds: 0 (immediate)              â”‚
â”‚    - scaleUp.policies: 100% or +4 pods per 15s                      â”‚
â”‚    - Result: 2 â†’ 6 â†’ 10 in ~15 seconds                              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stress Removed (08:42:56):**
```bash
kubectl delete pod api-66bd89b9bf-m24cz --grace-period=0 --force
```

**Scale-Down Sequence:**
```
TIMESTAMP   EVENT                 REPLICAS   REASON
08:45:00    (stabilization)      10         Waiting for 5-minute window
08:47:48    SuccessfulRescale    10 â†’ 5     All metrics below target
08:48:48    SuccessfulRescale    5 â†’ 3      All metrics below target
08:52:48    SuccessfulRescale    3 â†’ 2      All metrics below target (min reached)
```

**Scale-Down Analysis:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API SCALE-DOWN TIMELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  08:42:56  Stress removed, CPU drops to 1%                          â”‚
â”‚            â”‚                                                         â”‚
â”‚            â”œâ”€â”€â”€â”€ 5 minute stabilization window â”€â”€â”€â”€â”¤                â”‚
â”‚            â”‚                                        â”‚                â”‚
â”‚  08:47:48  First scale-down: 10 â†’ 5 pods                            â”‚
â”‚            (50% reduction per scaleDown policy)                      â”‚
â”‚            â”‚                                                         â”‚
â”‚  08:48:48  Second scale-down: 5 â†’ 3 pods                            â”‚
â”‚            (50% of 5 = 2.5, rounds to 3)                            â”‚
â”‚            â”‚                                                         â”‚
â”‚  08:52:48  Final scale-down: 3 â†’ 2 pods                             â”‚
â”‚            (reached minReplicas)                                     â”‚
â”‚                                                                      â”‚
â”‚  Total scale-down time: ~10 minutes                                  â”‚
â”‚  Configuration:                                                      â”‚
â”‚    - stabilizationWindowSeconds: 300 (5 minutes)                    â”‚
â”‚    - scaleDown policy: 50% per 60 seconds                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

##### Test 2: Gateway Service Scale-Up and Scale-Down

**Stress Test Started (08:43:31):**
```bash
kubectl exec gateway-67644498-2jv8j -- sh -c "for i in 1 2 3 4 5 6; do yes > /dev/null & done"
```

**Scale-Up Detected (08:49:46):**
```
NAME          REFERENCE            TARGETS                          REPLICAS
gateway-hpa   Deployment/gateway   cpu: 100%/50%, memory: 26%/70%   5

Pod Resource Usage:
NAME                     CPU(cores)   MEMORY(bytes)
gateway-67644498-2jv8j   501m         34Mi            (stressed pod)
gateway-67644498-499l5   1m           33Mi
gateway-67644498-kzhn7   1m           33Mi            (new)
gateway-67644498-lzjtv   0m           33Mi            (new)
gateway-67644498-z9mj9   1m           33Mi            (new)
```

**Gateway HPA Scaling Events:**
```
TIMESTAMP   EVENT                 REPLICAS   REASON
08:44:45    SuccessfulRescale    2 â†’ 4      cpu resource utilization above target
08:45:00    SuccessfulRescale    4 â†’ 5      cpu resource utilization above target
08:50:35    SuccessfulRescale    5 â†’ 3      All metrics below target
08:50:35    SuccessfulRescale    3 â†’ 2      All metrics below target
```

---

##### Complete Test Summary

**All Scaling Events (Chronological):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TIME      SERVICE    ACTION      REPLICAS   REASON                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  08:38:36  API        Scale Up    2 â†’ 6      CPU above target (250%)         â”‚
â”‚  08:38:51  API        Scale Up    6 â†’ 10     CPU above target                â”‚
â”‚  08:44:45  Gateway    Scale Up    2 â†’ 4      CPU above target (100%)         â”‚
â”‚  08:45:00  Gateway    Scale Up    4 â†’ 5      CPU above target                â”‚
â”‚  08:47:48  API        Scale Down  10 â†’ 5     All metrics below target        â”‚
â”‚  08:48:48  API        Scale Down  5 â†’ 3      All metrics below target        â”‚
â”‚  08:50:35  Gateway    Scale Down  5 â†’ 3      All metrics below target        â”‚
â”‚  08:50:35  Gateway    Scale Down  3 â†’ 2      All metrics below target        â”‚
â”‚  08:52:48  API        Scale Down  3 â†’ 2      All metrics below target        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Final State (08:53:08):**
```
NAME          REFERENCE            TARGETS                        MINPODS   MAXPODS   REPLICAS
api-hpa       Deployment/api       cpu: 1%/50%, memory: 16%/70%   2         10        2
gateway-hpa   Deployment/gateway   cpu: 1%/50%, memory: 26%/70%   2         5         2
product-hpa   Deployment/product   cpu: 1%/50%, memory: 17%/70%   1         5         1
worker-hpa    Deployment/worker    cpu: 1%/60%, memory: 8%/70%    2         15        2

Final Pod Counts: API=2, Gateway=2, Worker=2, Product=1
```

**Test Results Summary:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TEST RESULTS SUMMARY                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  âœ… API HPA Scale-Up:      PASSED (2 â†’ 10 in ~15 seconds)           â”‚
â”‚  âœ… API HPA Scale-Down:    PASSED (10 â†’ 2 in ~10 minutes)           â”‚
â”‚  âœ… Gateway HPA Scale-Up:  PASSED (2 â†’ 5 in ~15 seconds)            â”‚
â”‚  âœ… Gateway HPA Scale-Down: PASSED (5 â†’ 2 in ~5 minutes)            â”‚
â”‚  âœ… Metrics Server:        WORKING (accurate CPU/memory metrics)    â”‚
â”‚  âœ… Stabilization Window:  WORKING (5-minute delay before scale-down)â”‚
â”‚  âœ… Scale Policies:        WORKING (50% max scale-down per minute)  â”‚
â”‚                                                                      â”‚
â”‚  Key Observations:                                                   â”‚
â”‚  â€¢ Scale-up is immediate (stabilizationWindowSeconds: 0)            â”‚
â”‚  â€¢ Scale-down is gradual (5-min stabilization + 50% policy)         â”‚
â”‚  â€¢ New pods start receiving traffic within ~30 seconds               â”‚
â”‚  â€¢ CPU limits (500m) effectively cap stressed pod resource usage    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Verifying Test Success

```bash
# Check scaling events
kubectl describe hpa api-hpa | grep -A 15 Events

# Expected output:
#   Normal  SuccessfulRescale  New size: 6; reason: cpu resource utilization above target
#   Normal  SuccessfulRescale  New size: 10; reason: cpu resource utilization above target
#   Normal  SuccessfulRescale  New size: 5; reason: All metrics below target
#   Normal  SuccessfulRescale  New size: 3; reason: All metrics below target
#   Normal  SuccessfulRescale  New size: 2; reason: All metrics below target

# View all HPA scaling events
kubectl get events --sort-by='.lastTimestamp' --field-selector reason=SuccessfulRescale

# Monitor HPA in real-time
kubectl get hpa -w

# Check pod resource usage
kubectl top pods
```

---

#### Understanding the Test Results in Detail

##### How CPU Percentage is Calculated

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                CPU UTILIZATION CALCULATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Formula:                                                            â”‚
â”‚    CPU Utilization % = (Actual CPU Usage / CPU Request) Ã— 100       â”‚
â”‚                                                                      â”‚
â”‚  Example from API Test:                                              â”‚
â”‚    CPU Request: 100m (defined in deployment)                        â”‚
â”‚    Actual Usage: 500m (during stress - capped at limit)             â”‚
â”‚    Utilization: (500 / 100) Ã— 100 = 500%                            â”‚
â”‚                                                                      â”‚
â”‚  Note: Utilization CAN exceed 100% because:                         â”‚
â”‚    - Request is the "guaranteed" amount                             â”‚
â”‚    - Limit is the "maximum" amount (500m in our config)             â”‚
â”‚    - Pod can use up to limit if node has capacity                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### How Memory Percentage is Calculated

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                MEMORY UTILIZATION CALCULATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Formula:                                                            â”‚
â”‚    Memory Utilization % = (Actual Memory / Memory Request) Ã— 100    â”‚
â”‚                                                                      â”‚
â”‚  Example from API Test:                                              â”‚
â”‚    Memory Request: 128Mi                                            â”‚
â”‚    Actual Usage: 21Mi                                               â”‚
â”‚    Utilization: (21 / 128) Ã— 100 = 16%                              â”‚
â”‚                                                                      â”‚
â”‚  Note: Memory stayed low because our stress test only               â”‚
â”‚        consumed CPU, not memory.                                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### HPA Replica Calculation Formula

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                HPA SCALING ALGORITHM                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Formula:                                                            â”‚
â”‚    desiredReplicas = ceil[currentReplicas Ã— (currentMetric/target)] â”‚
â”‚                                                                      â”‚
â”‚  API Scale-Up Calculation:                                          â”‚
â”‚    Before stress:                                                    â”‚
â”‚      Pod 1: 1m CPU (1%)                                             â”‚
â”‚      Pod 2: 1m CPU (1%)                                             â”‚
â”‚      Average: 1%                                                     â”‚
â”‚                                                                      â”‚
â”‚    After stress applied:                                            â”‚
â”‚      Pod 1: 500m CPU (500% - capped at limit)                       â”‚
â”‚      Pod 2: 1m CPU (1%)                                             â”‚
â”‚      Average: (500 + 1) / 2 = 250.5m = 250%                         â”‚
â”‚                                                                      â”‚
â”‚    Calculation:                                                      â”‚
â”‚      desiredReplicas = ceil[2 Ã— (250 / 50)]                         â”‚
â”‚      desiredReplicas = ceil[2 Ã— 5]                                  â”‚
â”‚      desiredReplicas = ceil[10]                                     â”‚
â”‚      desiredReplicas = 10                                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### Why Scale-Up Was Fast (~15 seconds)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                SCALE-UP BEHAVIOR CONFIGURATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Configuration (from api-hpa.yaml):                                 â”‚
â”‚    behavior:                                                         â”‚
â”‚      scaleUp:                                                        â”‚
â”‚        stabilizationWindowSeconds: 0  # No waiting - immediate      â”‚
â”‚        policies:                                                     â”‚
â”‚        - type: Percent                                               â”‚
â”‚          value: 100                   # Can double pods              â”‚
â”‚          periodSeconds: 15                                           â”‚
â”‚        - type: Pods                                                  â”‚
â”‚          value: 4                     # Or add 4 pods                â”‚
â”‚          periodSeconds: 15                                           â”‚
â”‚        selectPolicy: Max              # Use whichever adds more      â”‚
â”‚                                                                      â”‚
â”‚  Result:                                                             â”‚
â”‚    - stabilizationWindowSeconds: 0 means react immediately          â”‚
â”‚    - 100% policy allows doubling: 2 â†’ 4 â†’ 8                         â”‚
â”‚    - +4 pods policy allows: 2 â†’ 6 â†’ 10                              â”‚
â”‚    - selectPolicy: Max chooses the larger increase                  â”‚
â”‚    - Total time: 2 â†’ 6 â†’ 10 in ~15 seconds                          â”‚
â”‚                                                                      â”‚
â”‚  Why Fast Scale-Up Matters:                                          â”‚
â”‚    - Traffic spikes need immediate response                         â”‚
â”‚    - Users shouldn't wait for pods to scale                         â”‚
â”‚    - Better to over-provision briefly than drop requests            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### Why Scale-Down Was Slow (~10 minutes)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                SCALE-DOWN BEHAVIOR CONFIGURATION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Configuration (from api-hpa.yaml):                                 â”‚
â”‚    behavior:                                                         â”‚
â”‚      scaleDown:                                                      â”‚
â”‚        stabilizationWindowSeconds: 300  # Wait 5 minutes first      â”‚
â”‚        policies:                                                     â”‚
â”‚        - type: Percent                                               â”‚
â”‚          value: 50                      # Max 50% reduction          â”‚
â”‚          periodSeconds: 60              # Per minute                 â”‚
â”‚                                                                      â”‚
â”‚  Timeline:                                                           â”‚
â”‚    08:42:56  Stress removed, CPU drops to 1%                        â”‚
â”‚    08:43-08:47  Stabilization window (5 minutes) - NO changes       â”‚
â”‚    08:47:48  First scale-down: 10 â†’ 5 (50% reduction)               â”‚
â”‚    08:48:48  Second scale-down: 5 â†’ 3 (50% = 2.5, rounds to 3)      â”‚
â”‚    08:52:48  Final scale-down: 3 â†’ 2 (reached minReplicas)          â”‚
â”‚                                                                      â”‚
â”‚  Why Slow Scale-Down Matters:                                        â”‚
â”‚    - Prevents "flapping" (rapid scale up/down cycles)               â”‚
â”‚    - Traffic dips might be temporary                                â”‚
â”‚    - Allows time for traffic patterns to stabilize                  â”‚
â”‚    - Avoids killing pods during brief low-traffic periods           â”‚
â”‚    - Gradual reduction prevents sudden capacity loss                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### Visual Timeline of API Scaling Test

```
                    API SERVICE SCALING TEST

    Replicas
       â”‚
    10 â”¤                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â”‚                â–ˆ               â–ˆ
     8 â”¤               â–ˆ                 â–ˆ
       â”‚              â–ˆ                   â–ˆ
     6 â”¤             â–ˆ                     â–ˆ
       â”‚            â–ˆ                       â–ˆ
     4 â”¤                                     â–ˆ
       â”‚                                      â–ˆ
     2 â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Time
         08:38    08:40    08:43    08:48    08:53

         â”‚        â”‚        â”‚        â”‚        â”‚
         â”‚        â”‚        â”‚        â”‚        â””â”€â”€ Final: 2 pods (min)
         â”‚        â”‚        â”‚        â””â”€â”€ Scale-down: 10â†’5â†’3â†’2
         â”‚        â”‚        â””â”€â”€ Stress removed
         â”‚        â””â”€â”€ Scale-up: 2â†’6â†’10 (max)
         â””â”€â”€ Stress started
```

##### Visual Timeline of Gateway Scaling Test

```
                    GATEWAY SERVICE SCALING TEST

    Replicas
       â”‚
     5 â”¤            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â”‚           â–ˆ                    â–ˆ
     4 â”¤          â–ˆ                      â–ˆ
       â”‚                                  â–ˆ
     3 â”¤                                   â–ˆ
       â”‚                                    â–ˆ
     2 â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Time
         08:43    08:45    08:50    08:51

         â”‚        â”‚        â”‚        â”‚
         â”‚        â”‚        â”‚        â””â”€â”€ Final: 2 pods (min)
         â”‚        â”‚        â””â”€â”€ Scale-down: 5â†’3â†’2
         â”‚        â””â”€â”€ Scale-up: 2â†’4â†’5 (max)
         â””â”€â”€ Stress started
```

##### Test Validation Matrix

| Test Case | Expected | Actual | Result |
|-----------|----------|--------|--------|
| API scale-up triggers at >50% CPU | Yes | Triggered at 250% | âœ… PASS |
| API scales to max (10) | Yes | Scaled to 10 | âœ… PASS |
| Scale-up is fast (<60s) | Yes | ~15 seconds | âœ… PASS |
| Stabilization window enforced | 5 min wait | 5 min observed | âœ… PASS |
| Scale-down is gradual (50%/min) | Yes | 10â†’5â†’3â†’2 | âœ… PASS |
| Returns to minReplicas | 2 pods | 2 pods | âœ… PASS |
| Gateway HPA works independently | Yes | Scaled 2â†’5â†’2 | âœ… PASS |
| Metrics Server provides data | Yes | CPU/Memory shown | âœ… PASS |

##### Practical Implications for Production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PRODUCTION IMPLICATIONS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. TRAFFIC SPIKES                                                   â”‚
â”‚     Your services will automatically handle traffic spikes by       â”‚
â”‚     adding pods within ~15-30 seconds. Users experience minimal     â”‚
â”‚     latency increase during scaling.                                â”‚
â”‚                                                                      â”‚
â”‚  2. COST EFFICIENCY                                                  â”‚
â”‚     Scale-down is gradual, preventing unnecessary resource waste    â”‚
â”‚     while avoiding premature pod termination. You only pay for      â”‚
â”‚     what you need.                                                  â”‚
â”‚                                                                      â”‚
â”‚  3. HIGH AVAILABILITY                                                â”‚
â”‚     Minimum replicas (2 for API/Gateway, 1 for Product) ensure      â”‚
â”‚     services stay available even during scale-down. No single       â”‚
â”‚     point of failure.                                               â”‚
â”‚                                                                      â”‚
â”‚  4. RESOURCE PROTECTION                                              â”‚
â”‚     CPU limits (500m) prevent any single pod from consuming all     â”‚
â”‚     node resources. Other pods and system components remain         â”‚
â”‚     responsive.                                                     â”‚
â”‚                                                                      â”‚
â”‚  5. PREDICTABLE BEHAVIOR                                             â”‚
â”‚     - Scale-up: Immediate (0s stabilization)                        â”‚
â”‚     - Scale-down: Gradual (5-min wait + 50% policy)                 â”‚
â”‚     - New pods receive traffic within ~30 seconds                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Production Checklist

- [ ] Metrics Server installed and running
- [ ] All deployments have resource requests/limits
- [ ] HPA manifests applied
- [ ] Stabilization windows configured
- [ ] Max replicas within VM capacity
- [ ] Monitoring/alerting configured
- [ ] Load testing completed

---

## Quick Reference

### Apply All Scaling Components

```bash
# 1. Install metrics server
kubectl apply -f manifests/hpa/metrics-server.yaml

# 2. Wait for metrics
kubectl wait --for=condition=available deployment/metrics-server -n kube-system --timeout=300s

# 3. Apply deployments (with resources)
kubectl apply -f manifests/prod-manifests/

# 4. Apply HPAs
kubectl apply -f manifests/hpa/

# 5. Verify
kubectl get hpa
kubectl top pods
```

### Kind Cluster Setup

For kind (Kubernetes in Docker) clusters:

```bash
# 1. Create cluster (if not exists)
kind create cluster --name microservices-prod

# 2. Switch context
kubectl config use-context kind-microservices-prod

# 3. Load images into kind (if using local images)
kind load docker-image <image-name> --name microservices-prod

# 4. Apply metrics server (includes --kubelet-insecure-tls)
kubectl apply -f manifests/hpa/metrics-server.yaml

# 5. Wait for metrics server
kubectl wait --for=condition=available deployment/metrics-server -n kube-system --timeout=300s

# 6. Deploy application with resources
kubectl apply -f manifests/prod-manifests/

# 7. Apply HPAs
kubectl apply -f manifests/hpa/

# 8. Verify everything is working
kubectl get hpa
kubectl top pods
```

**Note:** If metrics-server shows `ImagePullBackOff`, the image needs to be loaded into kind:
```bash
docker pull registry.k8s.io/metrics-server/metrics-server:v0.7.0
kind load docker-image registry.k8s.io/metrics-server/metrics-server:v0.7.0 --name microservices-prod
kubectl rollout restart deployment/metrics-server -n kube-system
```

### Remove Autoscaling

```bash
# Delete HPAs (pods remain at current count)
kubectl delete -f manifests/hpa/api-hpa.yaml
kubectl delete -f manifests/hpa/gateway-hpa.yaml
kubectl delete -f manifests/hpa/worker-hpa.yaml
kubectl delete -f manifests/hpa/product-hpa.yaml

# Or delete all HPAs
kubectl delete hpa --all
```

### Manual Scaling (Override HPA)

```bash
# Temporarily disable HPA and set replicas
kubectl scale deployment api --replicas=5

# Re-enable HPA control
kubectl apply -f manifests/hpa/api-hpa.yaml
```

---

## Additional Resources

- [Kubernetes HPA Documentation](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
- [K3s Documentation](https://docs.k3s.io/)
- [Autoscaling Design Proposals](https://github.com/kubernetes/design-proposals-archive/blob/main/autoscaling/horizontal-pod-autoscaler.md)
- [ArgoCD Documentation](https://argo-cd.readthedocs.io/)

---

## Monitoring with ArgoCD

ArgoCD provides GitOps-based continuous delivery for Kubernetes. This section explains how to monitor HPA and scaling activity through ArgoCD.

### Accessing ArgoCD UI

#### Step 1: Port-Forward to ArgoCD Server

```bash
# Forward ArgoCD server to local port 8080
kubectl port-forward svc/argocd-server -n argocd 8080:443
```

#### Step 2: Get Admin Credentials

```bash
# Get the initial admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# Username: admin
# Password: <output from above command>
```

#### Step 3: Access the UI

Open your browser and navigate to:
```
https://localhost:8080
```

> **Note**: You may see a certificate warning since ArgoCD uses a self-signed certificate by default. Accept the warning to proceed.

### Viewing HPA in ArgoCD

#### Understanding ArgoCD Application Structure

```
ArgoCD Application: async-microservices
â”‚
â”œâ”€â”€ Source Repository: https://github.com/Ridwan414/async-micro.git
â”œâ”€â”€ Path: manifests/
â”œâ”€â”€ Target Revision: HEAD
â”‚
â””â”€â”€ Managed Resources:
    â”œâ”€â”€ Deployments (api, gateway, product, worker)
    â”œâ”€â”€ Services (api-service, gateway-service, product-service, rabbitmq)
    â”œâ”€â”€ StatefulSets (rabbitmq)
    â”œâ”€â”€ HPAs (api-hpa, gateway-hpa, product-hpa, worker-hpa)  â† Scaling Resources
    â””â”€â”€ Other (PVC, ConfigMaps, etc.)
```

#### Viewing HPA Resources in ArgoCD UI

1. **Navigate to Application**
   - Click on `async-microservices` application
   - You'll see a visual graph of all managed resources

2. **Filter by Resource Type**
   - Use the filter dropdown to select "HorizontalPodAutoscaler"
   - This shows only HPA resources

3. **View HPA Details**
   - Click on any HPA resource (e.g., `api-hpa`)
   - View the YAML manifest, live status, and events

4. **Monitor Real-time Scaling**
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    ArgoCD Application View                       â”‚
   â”‚                                                                  â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚  â”‚ api-hpa  â”‚â”€â”€â”€â”€â†’â”‚   api    â”‚â”€â”€â”€â”€â†’â”‚  api-5d8f9b7c6d-xxxxx   â”‚ â”‚
   â”‚  â”‚ (HPA)    â”‚     â”‚(Deploy)  â”‚     â”‚  api-5d8f9b7c6d-yyyyy   â”‚ â”‚
   â”‚  â”‚          â”‚     â”‚          â”‚     â”‚  api-5d8f9b7c6d-zzzzz   â”‚ â”‚
   â”‚  â”‚CPU: 45%  â”‚     â”‚Replicas:3â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
   â”‚                                                                  â”‚
   â”‚  Status: â— Synced   Health: â— Healthy                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

5. **Check Sync Status**
   - **Synced**: HPA configuration matches the Git repository
   - **OutOfSync**: HPA has drifted from Git (manual changes or pending sync)
   - **Unknown**: ArgoCD cannot determine status

### ArgoCD CLI Commands

#### Install ArgoCD CLI

```bash
# Linux
curl -sSL -o argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
chmod +x argocd
sudo mv argocd /usr/local/bin/

# macOS
brew install argocd

# Windows (using chocolatey)
choco install argocd-cli
```

#### Login to ArgoCD

```bash
# Get the admin password
ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)

# Login (using port-forward)
argocd login localhost:8080 --username admin --password $ARGOCD_PASSWORD --insecure
```

#### View Application and HPA Status

```bash
# List all applications
argocd app list

# Get detailed application info
argocd app get async-microservices

# View specific resource (HPA)
argocd app resources async-microservices --kind HorizontalPodAutoscaler

# View live manifest of HPA
argocd app manifests async-microservices --source live | grep -A 50 "kind: HorizontalPodAutoscaler"
```

#### Monitor Application Events

```bash
# Watch application events in real-time
argocd app watch async-microservices

# View application history
argocd app history async-microservices

# View sync status
argocd app sync-status async-microservices
```

### Configuring ArgoCD to Manage HPAs

#### Option 1: Include HPA in Main Manifests Path

Ensure your HPA manifests are in the path monitored by ArgoCD:

```yaml
# ArgoCD Application spec
spec:
  source:
    repoURL: https://github.com/Ridwan414/async-micro.git
    path: manifests/prod-manifests  # Include HPA folder
    targetRevision: HEAD
```

#### Option 2: Use Multiple Sources (ArgoCD 2.6+)

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: async-microservices
  namespace: argocd
spec:
  project: default
  sources:
    - repoURL: https://github.com/Ridwan414/async-micro.git
      path: manifests/prod-manifests
      targetRevision: HEAD
    - repoURL: https://github.com/Ridwan414/async-micro.git
      path: manifests/hpa
      targetRevision: HEAD
  destination:
    server: https://kubernetes.default.svc
    namespace: default
```

#### Option 3: Separate Application for HPAs

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: async-microservices-hpa
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/Ridwan414/async-micro.git
    path: manifests/hpa
    targetRevision: HEAD
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

### Monitoring HPA via kubectl (Alternative)

While ArgoCD provides a great UI, you can also use kubectl directly:

```bash
# Watch HPA status in real-time
kubectl get hpa -w

# Detailed HPA information
kubectl describe hpa api-hpa

# View HPA with current metrics
kubectl get hpa -o custom-columns=\
NAME:.metadata.name,\
REFERENCE:.spec.scaleTargetRef.name,\
MIN:.spec.minReplicas,\
MAX:.spec.maxReplicas,\
CURRENT:.status.currentReplicas,\
CPU:.status.currentMetrics[0].resource.current.averageUtilization,\
MEMORY:.status.currentMetrics[1].resource.current.averageUtilization

# View HPA events
kubectl get events --field-selector involvedObject.kind=HorizontalPodAutoscaler
```

### Current Cluster Status

To check your current HPA status:

```bash
kubectl get hpa

# Expected output:
# NAME          REFERENCE            TARGETS                        MINPODS   MAXPODS   REPLICAS
# api-hpa       Deployment/api       cpu: 1%/50%, memory: 16%/70%   2         10        2
# gateway-hpa   Deployment/gateway   cpu: 1%/50%, memory: 26%/70%   2         5         2
# product-hpa   Deployment/product   cpu: 1%/50%, memory: 17%/70%   1         5         1
# worker-hpa    Deployment/worker    cpu: 1%/60%, memory: 8%/70%    2         15        2
```

### Troubleshooting ArgoCD + HPA

| Issue | Cause | Solution |
|-------|-------|----------|
| HPA not showing in ArgoCD | HPA path not in source | Update application source path |
| HPA shows OutOfSync | Manual changes made | Run `argocd app sync` |
| HPA shows Unknown metrics | Metrics server not running | Deploy metrics-server |
| HPA ignored by ArgoCD | Resource excluded in config | Check `resource.exclusions` in argocd-cm |

### Quick Access Commands

```bash
# One-liner to access ArgoCD UI
kubectl port-forward svc/argocd-server -n argocd 8080:443 &
echo "Access ArgoCD at: https://localhost:8080"
echo "Username: admin"
echo "Password: $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d)"
```

---

## Testing Autoscaling via ArgoCD UI (Remote Server)

This section provides step-by-step instructions for testing autoscaling on a remote server using ArgoCD UI without SSH access.

### Prerequisites

1. **ArgoCD UI accessible** via NodePort or Ingress
2. **HPA Application deployed** via ArgoCD
3. **Load Test Application deployed** via ArgoCD
4. **Staging environment running** with services

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ARGOCD APPLICATIONS                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ async-micro-hpa   â”‚    â”‚async-micro-stagingâ”‚    â”‚async-micro-load-  â”‚    â”‚
â”‚  â”‚                   â”‚    â”‚                   â”‚    â”‚      test         â”‚    â”‚
â”‚  â”‚ Deploys:          â”‚    â”‚ Deploys:          â”‚    â”‚ Deploys:          â”‚    â”‚
â”‚  â”‚ â€¢ api-hpa         â”‚    â”‚ â€¢ api deployment  â”‚    â”‚ â€¢ load-test-stagingâ”‚   â”‚
â”‚  â”‚ â€¢ gateway-hpa     â”‚    â”‚ â€¢ gateway deploy  â”‚    â”‚ â€¢ load-test-ab    â”‚    â”‚
â”‚  â”‚ â€¢ product-hpa     â”‚    â”‚ â€¢ product deploy  â”‚    â”‚ â€¢ spike-test      â”‚    â”‚
â”‚  â”‚ â€¢ worker-hpa      â”‚    â”‚ â€¢ worker deploy   â”‚    â”‚                   â”‚    â”‚
â”‚  â”‚ â€¢ metrics-server  â”‚    â”‚ â€¢ rabbitmq        â”‚    â”‚                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                        â”‚                        â”‚                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                    â”‚                                          â”‚
â”‚                                    â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         STAGING NAMESPACE                                â”‚ â”‚
â”‚  â”‚                                                                          â”‚ â”‚
â”‚  â”‚   Load Test Pods â”€â”€â”€â”€â”€â”€â–¶ Gateway â”€â”€â”€â”€â”€â”€â–¶ API â”€â”€â”€â”€â”€â”€â–¶ RabbitMQ           â”‚ â”‚
â”‚  â”‚        â”‚                    â”‚              â”‚             â”‚               â”‚ â”‚
â”‚  â”‚        â”‚                    â”‚              â”‚             â–¼               â”‚ â”‚
â”‚  â”‚        â”‚                    â”‚              â”‚         Worker              â”‚ â”‚
â”‚  â”‚        â”‚                    â”‚              â”‚                             â”‚ â”‚
â”‚  â”‚        â”‚                    â–¼              â–¼                             â”‚ â”‚
â”‚  â”‚        â”‚               HPA monitors CPU/Memory                          â”‚ â”‚
â”‚  â”‚        â”‚                    â”‚              â”‚                             â”‚ â”‚
â”‚  â”‚        â”‚                    â–¼              â–¼                             â”‚ â”‚
â”‚  â”‚        â”‚            Scale Up/Down pods automatically                    â”‚ â”‚
â”‚  â”‚        â”‚                                                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                                                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traffic Flow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 1: Access ArgoCD UI

**Remote Server URL:** `http://<server-ip>:30588`

**Example:** `http://103.191.50.49:30588`

**Credentials:**
- Username: `admin`
- Password: Get from secret or use saved password

### Step 2: Deploy HPA Application

If `async-micro-hpa` doesn't exist, create it:

1. Click **"+ NEW APP"** in ArgoCD UI
2. Click **"EDIT AS YAML"**
3. Clear all content and paste:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: async-micro-hpa
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/Ridwan414/async-micro.git
    targetRevision: stag
    path: manifests/hpa
  destination:
    server: https://kubernetes.default.svc
    namespace: staging
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

4. Click **"SAVE"** â†’ **"CREATE"**
5. Click **"SYNC"** â†’ **"SYNCHRONIZE"**

### Step 3: Verify HPA Deployment

After syncing, click on **async-micro-hpa** app. You should see:

```
async-micro-hpa (Healthy, Synced)
â”œâ”€â”€ HorizontalPodAutoscaler/api-hpa âœ“
â”œâ”€â”€ HorizontalPodAutoscaler/gateway-hpa âœ“
â”œâ”€â”€ HorizontalPodAutoscaler/product-hpa âœ“
â”œâ”€â”€ HorizontalPodAutoscaler/worker-hpa âœ“
â””â”€â”€ Deployment/metrics-server (kube-system) âœ“
```

### Step 4: Deploy Load Test Application

If `async-micro-load-test` doesn't exist, create it:

1. Click **"+ NEW APP"**
2. Click **"EDIT AS YAML"**
3. Paste:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: async-micro-load-test
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/Ridwan414/async-micro.git
    targetRevision: stag
    path: load-test
  destination:
    server: https://kubernetes.default.svc
    namespace: staging
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
```

4. Click **"SAVE"** â†’ **"CREATE"**

### Step 5: Trigger Load Test

1. Click on **async-micro-load-test** app
2. Click **"SYNC"** â†’ **"SYNCHRONIZE"**
3. Watch the Jobs deploy:

```
async-micro-load-test (Healthy, Synced)
â”œâ”€â”€ Job/load-test-staging
â”‚   â””â”€â”€ Healthy 10 pods (running load)
â”œâ”€â”€ Job/load-test-ab
â”‚   â””â”€â”€ Healthy 5 pods (running load)
â””â”€â”€ Job/spike-test-staging
    â””â”€â”€ Healthy 5 pods (running load)
```

### Step 6: Monitor Scaling in Real-Time

1. Go back to **Applications** list
2. Click on **async-micro-staging**
3. Click **"REFRESH"** dropdown â†’ Enable **"Auto-Refresh"**
4. Watch the deployment replicas change!

**What to Look For:**

| Time | What You'll See |
|------|-----------------|
| 0-2 min | Load test pods starting, services receiving traffic |
| 2-5 min | HPA detects high CPU, triggers scale-up |
| 5-10 min | Pods increasing: API 2â†’10, Gateway 2â†’5, Product 1â†’5 |
| 10-15 min | Load test completes |
| 15-25 min | Stabilization window (pods stay high) |
| 25+ min | Gradual scale-down back to minimum |

### Step 7: View Scaling Details

**Option A: Click on a Deployment**

1. In **async-micro-staging**, click on `api` deployment
2. View the **SUMMARY** tab:
   - Current replicas
   - Desired replicas
   - Pod status

**Option B: Click on an HPA (if visible)**

1. In **async-micro-hpa**, click on `api-hpa`
2. View the **LIVE MANIFEST** tab to see current metrics

### Step 8: Re-run Load Test

To test scaling again:

1. Go to **async-micro-load-test** app
2. Click on each Job â†’ Click **"DELETE"**
3. Click **"SYNC"** â†’ **"SYNCHRONIZE"** to recreate jobs

### Visual Guide: What Scaling Looks Like in ArgoCD

**Before Load Test:**
```
async-micro-staging
â”œâ”€â”€ api (deploy)
â”‚   â””â”€â”€ api-xxxxx âœ“ (1 pod)
â”œâ”€â”€ gateway (deploy)
â”‚   â””â”€â”€ gateway-xxxxx âœ“ (2 pods)
â”œâ”€â”€ product (deploy)
â”‚   â””â”€â”€ product-xxxxx âœ“ (1 pod)
â””â”€â”€ worker (deploy)
    â””â”€â”€ worker-xxxxx âœ“ (2 pods)
```

**During Load Test (After ~5 minutes):**
```
async-micro-staging
â”œâ”€â”€ api (deploy)
â”‚   â”œâ”€â”€ api-xxxxx âœ“
â”‚   â”œâ”€â”€ api-yyyyy âœ“
â”‚   â”œâ”€â”€ api-zzzzz âœ“
â”‚   â”œâ”€â”€ api-aaaaa âœ“
â”‚   â””â”€â”€ ... (scaling to 10 pods) ðŸ“ˆ
â”œâ”€â”€ gateway (deploy)
â”‚   â”œâ”€â”€ gateway-xxxxx âœ“
â”‚   â”œâ”€â”€ gateway-yyyyy âœ“
â”‚   â”œâ”€â”€ gateway-zzzzz âœ“
â”‚   â””â”€â”€ ... (scaling to 5 pods) ðŸ“ˆ
â”œâ”€â”€ product (deploy)
â”‚   â”œâ”€â”€ product-xxxxx âœ“
â”‚   â””â”€â”€ ... (scaling to 5 pods) ðŸ“ˆ
â””â”€â”€ worker (deploy)
    â””â”€â”€ worker-xxxxx âœ“ (2 pods - no direct load)
```

**After Load Test (After ~25 minutes):**
```
async-micro-staging
â”œâ”€â”€ api (deploy)
â”‚   â””â”€â”€ api-xxxxx âœ“ (back to 2 pods) ðŸ“‰
â”œâ”€â”€ gateway (deploy)
â”‚   â””â”€â”€ gateway-xxxxx âœ“ (back to 2 pods) ðŸ“‰
â”œâ”€â”€ product (deploy)
â”‚   â””â”€â”€ product-xxxxx âœ“ (back to 1 pod) ðŸ“‰
â””â”€â”€ worker (deploy)
    â””â”€â”€ worker-xxxxx âœ“ (2 pods)
```

### Troubleshooting

#### Issue: Pods Not Scaling

**Possible Causes:**
1. HPA not deployed
2. Metrics server not running
3. Deployments missing resource requests

**Solution via ArgoCD:**
1. Check **async-micro-hpa** app exists and is **Synced**
2. In **async-micro-hpa**, verify `metrics-server` deployment is healthy
3. Check **async-micro-staging** deployments have resource limits

#### Issue: Load Test Not Running

**Possible Causes:**
1. Jobs already completed (Jobs run once)
2. Gateway service not accessible from load test pods

**Solution:**
1. Delete completed Jobs and re-sync
2. Check gateway-service exists in staging namespace

#### Issue: HPA Shows Unknown Metrics

**Cause:** Metrics server not collecting data yet

**Solution:** Wait 1-2 minutes after metrics-server deploys, then refresh

### Expected Scaling Behavior

| Service | Min | Max | CPU Target | Expected Scale-Up |
|---------|-----|-----|------------|-------------------|
| API | 2 | 10 | 50% | 2 â†’ 6 â†’ 10 |
| Gateway | 2 | 5 | 30% | 2 â†’ 4 â†’ 5 |
| Product | 1 | 5 | 50% | 1 â†’ 3 â†’ 5 |
| Worker | 2 | 15 | 60% | Minimal (no direct HTTP load) |

### Timeline Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUTOSCALING TEST TIMELINE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  T+0 min    â”‚ Sync load-test app â†’ Jobs created â†’ 20 pods start             â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+2 min    â”‚ Load hits services â†’ CPU increases                            â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+3 min    â”‚ HPA detects high CPU â†’ triggers scale-up                      â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+5 min    â”‚ New pods running â†’ API: 10, Gateway: 5, Product: 5            â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+10 min   â”‚ Load test Jobs complete â†’ load decreases                      â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+15 min   â”‚ Stabilization window (5 min) â†’ no scale-down yet              â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+20 min   â”‚ Scale-down starts â†’ 50% reduction per minute                  â”‚
â”‚             â”‚                                                                â”‚
â”‚  T+25 min   â”‚ Back to minimum â†’ API: 2, Gateway: 2, Product: 1              â”‚
â”‚             â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Reference: ArgoCD Apps for Autoscaling

| Application | Purpose | Path | Auto-Sync |
|-------------|---------|------|-----------|
| async-micro-hpa | Deploy HPAs & metrics-server | `manifests/hpa` | Yes |
| async-micro-staging | Deploy services | `manifests/staging-manifests` | Yes |
| async-micro-load-test | Run load tests | `load-test` | No (manual) |
